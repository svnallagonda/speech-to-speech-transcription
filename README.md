# Speech-to-Speech Transcription and Translation

![Build Status](https://img.shields.io/badge/status-active-brightgreen) ![License](https://img.shields.io/badge/license-MIT-blue) ![Python](https://img.shields.io/badge/python-3.8%2B-yellow)

An attractive, easy-to-read repository README for a speech-to-speech transcription/translation project. This project converts spoken audio from one language into spoken audio in another, supporting both real-time streaming and offline batch workflows.

---

## Key Features

- End-to-end speech-to-speech pipeline: ASR -> MT -> TTS
- Streaming (low-latency) and batch (high-quality) modes
- Modular: swap ASR/MT/TTS models and runtimes easily
- Multilingual: configurable source/target language pairs
- Dockerized for reproducible deployment
- REST & WebSocket APIs for integration

---

## Why this project?

This repository makes it straightforward to prototype, evaluate, and deploy speech-to-speech systems. It focuses on modularity so you can experiment with different ASR, MT, and TTS models, or train end-to-end S2S systems (e.g., Translatotron-style) without changing the whole stack.

---

## Quickstart

Requirements:
- Python 3.8+
- PyTorch
- FFmpeg (for audio processing)
- (Optional) Docker

Install:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Offline example (translate a single file):

```bash
python scripts/translate_file.py --input audio/source.wav --src en --tgt es --output audio/translated_es.wav
```

Run the server (development):

```bash
python -m server --config configs/local.yml
# or using Docker
docker build -t s2s-translator:latest .
docker run -p 8000:8000 s2s-translator:latest
```

Streaming (WebSocket):

```bash
python scripts/stream_server.py --config configs/stream.yml
# connect a client to ws://localhost:8000/stream
```

---

## Architecture Overview

1. Input: live microphone or audio file
2. ASR: speech -> text (streaming or batch)
3. MT: text -> text (supports incremental translation)
4. TTS: text -> synthetic speech
5. Optional: voice conversion / prosody transfer to preserve speaker characteristics

Pipelines:
- Streaming: chunked audio -> streaming ASR -> incremental MT -> streaming TTS
- Offline: full-audio -> batch ASR -> MT on transcript -> high-quality TTS

---

## Suggested Models & Tools

- ASR: Whisper, Wav2Vec 2.0, Kaldi
- MT: MarianMT, mBART, fairseq
- TTS: VITS, Tacotron2, FastSpeech2
- End-to-end S2S: Translatotron, Translatotron2
- Libraries: PyTorch, Hugging Face Transformers, torchaudio
- Deployment: ONNX, TensorRT, Docker

---

## Configuration

Use YAML files in the `configs/` directory to control models and pipeline behavior.

Example `configs/models.yml`:

```yaml
asr:
  type: whisper
  model: small
mt:
  type: marian
  model: Helsinki-NLP/opus-mt-en-es
tts:
  type: vits
  model_path: /models/vits_en_es.pth
```

---

## Evaluation Metrics

- ASR: WER (Word Error Rate)
- MT: BLEU or chrF
- TTS: MOS (Mean Opinion Score), objective mel-spectrogram metrics
- End-to-end: human rating for intelligibility, fidelity, and latency

---

## Datasets

- CoVoST, MuST-C (speech translation)
- LibriSpeech (ASR pretraining)
- Common Voice (multilingual speech)

---

## Performance & Latency Tips

- Use streaming ASR and incremental MT for low latency
- Quantize models or convert to ONNX/TensorRT for faster inference
- Pipeline ASR/MT/TTS in separate processes

---

## Security & Privacy

- Minimize audio retention and delete temp files
- Secure APIs with API keys / OAuth2
- Consider on-device inference for sensitive audio

---

## Contributing

Contributions welcome! Suggested flow:

1. Fork the repo
2. Create a branch: `feature/your-change`
3. Open a PR with description and tests/examples

See CONTRIBUTING.md for more.

---

## License

This project uses the MIT License â€” update as appropriate for your repo and include upstream model licenses where required.

---

## Maintainers

- Maintainer: svnallagonda <>
- Repo: https://github.com/svnallagonda/speech-to-speech-transcription
